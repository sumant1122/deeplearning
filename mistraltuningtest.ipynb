{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5111,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":3899}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install accelerate bitsandbytes peft trl datasets huggingface_hub","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install -U datasets","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"notebook_login()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,HfArgumentParser,TrainingArguments,pipeline, logging\nfrom peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\nimport os,torch, wandb\nfrom datasets import load_dataset\nfrom trl import SFTTrainer","metadata":{"execution":{"iopub.status.busy":"2024-02-04T17:27:50.183344Z","iopub.execute_input":"2024-02-04T17:27:50.183785Z","iopub.status.idle":"2024-02-04T17:27:59.890494Z","shell.execute_reply.started":"2024-02-04T17:27:50.183751Z","shell.execute_reply":"2024-02-04T17:27:59.889691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_hf = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")","metadata":{"execution":{"iopub.status.busy":"2024-02-04T17:28:03.930574Z","iopub.execute_input":"2024-02-04T17:28:03.931602Z","iopub.status.idle":"2024-02-04T17:28:04.143592Z","shell.execute_reply.started":"2024-02-04T17:28:03.931569Z","shell.execute_reply":"2024-02-04T17:28:04.142791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_model = \"/kaggle/input/mistral/pytorch/7b-v0.1-hf/1\"\ndataset_name = \"paperplaneflyr/recepies_reduced_3.0_1K\"\nnew_model = \"mistral_7b_recepies_reduced_3.0_1K\"","metadata":{"execution":{"iopub.status.busy":"2024-02-04T18:28:10.802579Z","iopub.execute_input":"2024-02-04T18:28:10.803312Z","iopub.status.idle":"2024-02-04T18:28:10.810624Z","shell.execute_reply.started":"2024-02-04T18:28:10.803276Z","shell.execute_reply":"2024-02-04T18:28:10.809248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Importing the dataset\ndataset = load_dataset(dataset_name, split=\"train\")\ndataset[\"text\"][100]","metadata":{"execution":{"iopub.status.busy":"2024-02-04T17:28:08.901940Z","iopub.execute_input":"2024-02-04T17:28:08.902860Z","iopub.status.idle":"2024-02-04T17:28:10.939835Z","shell.execute_reply.started":"2024-02-04T17:28:08.902819Z","shell.execute_reply":"2024-02-04T17:28:10.938882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.shape","metadata":{"execution":{"iopub.status.busy":"2024-02-04T17:28:13.772674Z","iopub.execute_input":"2024-02-04T17:28:13.773598Z","iopub.status.idle":"2024-02-04T17:28:13.779395Z","shell.execute_reply.started":"2024-02-04T17:28:13.773564Z","shell.execute_reply":"2024-02-04T17:28:13.778434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bnb_config = BitsAndBytesConfig(  \n    load_in_4bit= True,\n    bnb_4bit_quant_type= \"nf4\",\n    bnb_4bit_compute_dtype= torch.bfloat16,\n    bnb_4bit_use_double_quant= False,\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n        base_model,\n        load_in_4bit=True,\n        quantization_config=bnb_config,\n        torch_dtype=torch.bfloat16,\n        device_map=\"auto\",\n        trust_remote_code=True,\n)\nmodel.config.use_cache = False # silence the warnings\nmodel.config.pretraining_tp = 1\nmodel.gradient_checkpointing_enable()","metadata":{"execution":{"iopub.status.busy":"2024-02-04T17:28:16.719908Z","iopub.execute_input":"2024-02-04T17:28:16.720566Z","iopub.status.idle":"2024-02-04T17:30:24.636147Z","shell.execute_reply.started":"2024-02-04T17:28:16.720533Z","shell.execute_reply":"2024-02-04T17:30:24.635146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\ntokenizer.padding_side = 'right'\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.add_eos_token = True\ntokenizer.add_bos_token, tokenizer.add_eos_token","metadata":{"execution":{"iopub.status.busy":"2024-02-04T17:30:50.105398Z","iopub.execute_input":"2024-02-04T17:30:50.105792Z","iopub.status.idle":"2024-02-04T17:30:50.209644Z","shell.execute_reply.started":"2024-02-04T17:30:50.105764Z","shell.execute_reply":"2024-02-04T17:30:50.208390Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = prepare_model_for_kbit_training(model)\npeft_config = LoraConfig(\n    lora_alpha=16,\n    lora_dropout=0.1,\n    r=64,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\"]\n)\nmodel = get_peft_model(model, peft_config)","metadata":{"execution":{"iopub.status.busy":"2024-02-04T17:31:03.678908Z","iopub.execute_input":"2024-02-04T17:31:03.679278Z","iopub.status.idle":"2024-02-04T17:31:05.016702Z","shell.execute_reply.started":"2024-02-04T17:31:03.679250Z","shell.execute_reply":"2024-02-04T17:31:05.015680Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb.login()\nrun = wandb.init(\n    project='Fine tuning mistral 7B', \n    job_type=\"training\", \n    anonymous=\"allow\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-02-04T17:31:08.246284Z","iopub.execute_input":"2024-02-04T17:31:08.247213Z","iopub.status.idle":"2024-02-04T17:31:41.468393Z","shell.execute_reply.started":"2024-02-04T17:31:08.247178Z","shell.execute_reply":"2024-02-04T17:31:41.467525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_arguments = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=1,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=1,\n    optim=\"paged_adamw_32bit\",\n    save_steps=25,\n    logging_steps=25,\n    learning_rate=2e-4,\n    weight_decay=0.001,\n    fp16=False,\n    bf16=False,\n    max_grad_norm=0.3,\n    max_steps=-1,\n    warmup_ratio=0.03,\n    group_by_length=True,\n    lr_scheduler_type=\"constant\",\n    report_to=\"wandb\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-02-04T17:31:54.084067Z","iopub.execute_input":"2024-02-04T17:31:54.084553Z","iopub.status.idle":"2024-02-04T17:31:54.092888Z","shell.execute_reply.started":"2024-02-04T17:31:54.084517Z","shell.execute_reply":"2024-02-04T17:31:54.091960Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    peft_config=peft_config,\n    max_seq_length= None,\n    dataset_text_field=\"text\",\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing= False,\n)","metadata":{"execution":{"iopub.status.busy":"2024-02-04T17:31:56.739284Z","iopub.execute_input":"2024-02-04T17:31:56.739681Z","iopub.status.idle":"2024-02-04T17:31:56.787325Z","shell.execute_reply.started":"2024-02-04T17:31:56.739649Z","shell.execute_reply":"2024-02-04T17:31:56.786326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-02-04T17:32:08.448367Z","iopub.execute_input":"2024-02-04T17:32:08.449216Z","iopub.status.idle":"2024-02-04T17:58:38.295069Z","shell.execute_reply.started":"2024-02-04T17:32:08.449182Z","shell.execute_reply":"2024-02-04T17:58:38.294040Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.model.save_pretrained(new_model)\nwandb.finish()\nmodel.config.use_cache = True","metadata":{"execution":{"iopub.status.busy":"2024-02-04T17:58:53.525582Z","iopub.execute_input":"2024-02-04T17:58:53.525957Z","iopub.status.idle":"2024-02-04T17:58:58.000568Z","shell.execute_reply.started":"2024-02-04T17:58:53.525932Z","shell.execute_reply":"2024-02-04T17:58:57.999742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.model.push_to_hub(new_model, use_temp_dir=False)","metadata":{"execution":{"iopub.status.busy":"2024-02-04T17:59:00.909341Z","iopub.execute_input":"2024-02-04T17:59:00.909751Z","iopub.status.idle":"2024-02-04T17:59:18.273279Z","shell.execute_reply.started":"2024-02-04T17:59:00.909721Z","shell.execute_reply":"2024-02-04T17:59:18.272279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logging.set_verbosity(logging.CRITICAL)\n\nprompt = \"How to make cheese cake?\"\npipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\nresult = pipe(f\"<s>[INST] {prompt} [/INST]\")\nprint(result[0]['generated_text'])","metadata":{"execution":{"iopub.status.busy":"2024-02-04T18:03:23.246815Z","iopub.execute_input":"2024-02-04T18:03:23.247226Z","iopub.status.idle":"2024-02-04T18:06:13.916582Z","shell.execute_reply.started":"2024-02-04T18:03:23.247196Z","shell.execute_reply":"2024-02-04T18:06:13.915579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Merge**","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer,pipeline \nfrom peft import PeftModel\nimport torch","metadata":{"execution":{"iopub.status.busy":"2024-02-04T18:27:36.229685Z","iopub.execute_input":"2024-02-04T18:27:36.230446Z","iopub.status.idle":"2024-02-04T18:27:55.310476Z","shell.execute_reply.started":"2024-02-04T18:27:36.230408Z","shell.execute_reply":"2024-02-04T18:27:55.309567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm -rf /kaggle/working/offload","metadata":{"execution":{"iopub.status.busy":"2024-02-04T18:26:55.851686Z","iopub.execute_input":"2024-02-04T18:26:55.852068Z","iopub.status.idle":"2024-02-04T18:26:57.086817Z","shell.execute_reply.started":"2024-02-04T18:26:55.852037Z","shell.execute_reply":"2024-02-04T18:26:57.085572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_model_reload = AutoModelForCausalLM.from_pretrained(\n        base_model,\n        return_dict=True,\n        low_cpu_mem_usage=True,\n        device_map=\"auto\",\n        trust_remote_code=True,\n        offload_folder = \"offload/\"\n)\n\nmodel = PeftModel.from_pretrained(base_model_reload, new_model,offload_folder = \"offload/\")\n","metadata":{"execution":{"iopub.status.busy":"2024-02-04T18:28:22.338468Z","iopub.execute_input":"2024-02-04T18:28:22.339184Z","iopub.status.idle":"2024-02-04T18:31:17.321359Z","shell.execute_reply.started":"2024-02-04T18:28:22.339153Z","shell.execute_reply":"2024-02-04T18:31:17.318976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"","metadata":{"execution":{"iopub.status.busy":"2024-02-04T18:32:16.540019Z","iopub.execute_input":"2024-02-04T18:32:16.540898Z","iopub.status.idle":"2024-02-04T18:32:16.616331Z","shell.execute_reply.started":"2024-02-04T18:32:16.540860Z","shell.execute_reply":"2024-02-04T18:32:16.615543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipe = pipeline(\n    \"text-generation\", \n    model=model, \n    tokenizer = tokenizer, \n    torch_dtype=torch.bfloat16, \n    device_map=\"auto\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-02-04T18:32:18.965341Z","iopub.execute_input":"2024-02-04T18:32:18.966227Z","iopub.status.idle":"2024-02-04T18:32:18.972044Z","shell.execute_reply.started":"2024-02-04T18:32:18.966193Z","shell.execute_reply":"2024-02-04T18:32:18.971026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt = \"How to make cheese cake\"\n\nsequences = pipe(\n    f\"<s>[INST] {prompt} [/INST]\",\n    do_sample=True,\n    max_new_tokens=100, \n    temperature=0.7, \n    top_k=50, \n    top_p=0.95,\n    num_return_sequences=1,\n)\nprint(sequences[0]['generated_text'])","metadata":{"execution":{"iopub.status.busy":"2024-02-04T18:39:09.382004Z","iopub.execute_input":"2024-02-04T18:39:09.382699Z","iopub.status.idle":"2024-02-04T18:40:44.718735Z","shell.execute_reply.started":"2024-02-04T18:39:09.382663Z","shell.execute_reply":"2024-02-04T18:40:44.716636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = model.merge_and_unload()\n\nmodel.push_to_hub(new_model, use_temp_dir=False)\ntokenizer.push_to_hub(new_model, use_temp_dir=False)","metadata":{},"execution_count":null,"outputs":[]}]}